import pandas as pd 
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import metrics

#Load Dataset
hotel_dataset = pd.read_csv('Hotels.csv')

#Define explanatory variables and a dependant variable
X = hotel_dataset[['Hotel_star_rating', 'Distance', 'Customer_rating', 'Squares']]

y = hotel_dataset['Price(BAM)']

#Divide the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=.40, random_state=101)

#The idea for the manual training was taken from the code on the regression material provided by Benjamin Vald√©s on https://drive.google.com/file/d/1Z_jkDP45zPIsIWh2e8Q4txERhnrHPSu1/view?usp=sharing
#The definitions will be kept as they were to keep the author's original comments

__errors__= [];  #global variable to store the errors/loss for visualisation

def h(params, sample):
	"""This evaluates a generic linear function h(x) with current parameters.  h stands for hypothesis

	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		sample (lst) a list containing the values of a sample 

	Returns:
		Evaluation of h(x)
	"""
	acum = 0
	for i in range(len(params)):
		acum = acum + params[i]*sample[i]  #evaluates h(x) = a+bx1+cx2+ ... nxn.. 
	return acum

def show_errors(params, samples,y):
	"""Appends the errors/loss that are generated by the estimated values of h and the real value y
	
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		samples (lst) a 2 dimensional list containing the input samples 
		y (lst) a list containing the corresponding real result for each sample
	
	"""
	global __errors__
	error_acum =0
#	print("transposed samples") 
#	print(samples)
	for i in range(len(samples)):
		hyp = h(params,samples[i])
		#print( "hyp  %f  y %f " % (hyp,  y[i]))   
		error=hyp-y[i]
		error_acum=+error**2 # this error is the original cost function, (the one used to make updates in GD is the derivated verssion of this formula)
	mean_error_param=error_acum/len(samples)
	__errors__.append(mean_error_param)

def GD(params, samples, y, alfa):
	"""Gradient Descent algorithm 
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		samples (lst) a 2 dimensional list containing the input samples 
		y (lst) a list containing the corresponding real result for each sample
		alfa(float) the learning rate
	Returns:
		temp(lst) a list with the new values for the parameters after 1 run of the sample set
	"""
	temp = list(params)
	general_error=0
	for j in range(len(params)):
		acum =0; error_acum=0
		for i in range(len(samples)):
			error = h(params,samples[i]) - y[i]
			acum = acum + error*samples[i][j]  #Sumatory part of the Gradient Descent formula for linear Regression.
		temp[j] = params[j] - alfa*(1/len(samples))*acum  #Subtraction of original parameter value with learning rate included.
	return temp

def scaling(samples):
	"""Normalizes sample values so that gradient descent can converge
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
	Returns:
		samples(lst) a list with the normalized version of the original samples
	"""
	acum =0
	samples = np.asarray(samples).T.tolist() 
	for i in range(1,len(samples)):	
		for j in range(len(samples[i])):
			acum=+ samples[i][j]
		avg = acum/(len(samples[i]))
		max_val = max(samples[i])
		#print("avg %f" % avg)
		#print(max_val)
		for j in range(len(samples[i])):
			#print(samples[i][j])
			samples[i][j] = (samples[i][j] - avg)/max_val  #Mean scaling
	return np.asarray(samples).T.tolist()

#Defining params and establishing samples with the training x and y values
params = [0,0,0,0]
samples = X_train.values[1:].tolist()
y = y_train[1:].tolist()

alfa =.01  #  learning rate
for i in range(len(samples)):
	if isinstance(samples[i], list):
		samples[i]=  [1]+samples[i]
	else:
		samples[i]=  [1,samples[i]]
##print ("original samples:")
##print (samples)
samples = scaling(samples)
##print ("scaled samples:")
##print (samples)

#Epochs or cycles

epochs = 0

while True:  #  run gradient descent until local minima is reached
	oldparams = list(params)
	#print (params)
	params=GD(params, samples,y,alfa)	
	show_errors(params, samples, y)  #only used to show errors, it is not used in calculation
	#print (params)
	epochs = epochs + 1
	if(oldparams == params or epochs == 2):   #  local minima is found when there is no further improvement
		#print ("samples:")
		#print(samples)
		print ("final params:")
		print (params)
		break

#use this to generate a graph of the errors/loss so we can see whats going on (diagnostics)
#import matplotlib.pyplot as plt  
#plt.plot(__errors__)
#plt.show()

#Delimit test size
tests = X_test.values.tolist()

#Same procedure is done like in training to scale but with the test set
for i in range(len(tests)):
	if isinstance(tests[i], list):
		tests[i]=  [1]+tests[i]
	else:
		tests[i]=  [1,tests[i]]

tests = scaling(tests)

#Predictions
cols = ["Hotel_star_rating", "Distance", "Customer_rating", "Squares"]

predictions = []

for i in range(len(tests)):
    predictions.append(h(params, tests[i]))

#Predictions in a list
y_test = y_test.tolist()

print('MAE:', metrics.mean_absolute_error(y_test, predictions)) 
print('MSE:', metrics.mean_squared_error(y_test, predictions)) 
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))
print('Coeff:', metrics.r2_score(y_test, predictions)) 
#Predicting the price of a new hotel

hotel_star_rating = float(input("Rating of the hotel (From 1 to 5): "))
distance = float(input("Distance from the hotel to midtown (in mts): "))
customer_rating = float(input("Public rating by users (From 1 to 10): "))
squares = float(input("How big is the room? (in square meters) "))
input = []
input.append(hotel_star_rating)
input.append(distance)
input.append(customer_rating)
input.append(squares)

prediction = h(params, input)

#print(prediction)
price = round(prediction)
print("\nThe price of the average night in the hotel will be of %i BAM " %price)
print("\nor %i MXN" %round(price * 12.4))